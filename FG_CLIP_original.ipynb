{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22fca1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required packages\n",
    "!pip install -U transformers faiss-gpu torch Pillay tqdm ipywidgets\n",
    "!pip install -U git+https://github.com/360CVGroup/FG-CLIP.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce555db",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "IMAGE_ROOT = Path(\"debbiedebrauwer/Spanje\")  # Update as needed\n",
    "MODEL_NAME = \"qihoo360/fg-clip-large\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32\n",
    "QDRANT_URL = \"https://cfd9b2d8-fc05-42a9-a872-8c3d26d0c400.eu-central-1-0.aws.cloud.qdrant.io:6333\"\n",
    "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.mxDtOqWAzg6CtZqBdzl6nbUAMkH8rKsExwL-EKbLRf8\"\n",
    "COLLECTION_NAME = \"film_locations\"\n",
    "JSON_PATH = \"export_studioscott.json\"\n",
    "\n",
    "# --- LOAD FG-CLIP MODEL ---\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c716d8b",
   "metadata": {},
   "source": [
    "## Vector Embedder (turns images into vector embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9baeb00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633a7e5c02004319953a142ca1b3c20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- PARSE JSON: BUILD LOCATION-LEVEL METADATA MAP ---\n",
    "def build_location_metadata(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    # Map location_id to title, url, and list of rooms\n",
    "    location_meta = {}\n",
    "    for loc in data:\n",
    "        loc_id = loc[\"id\"]\n",
    "        loc_title = loc[\"title\"]\n",
    "        loc_url = loc.get(\"url\", \"\")\n",
    "        rooms = [child.get(\"place\", \"unknown\") for child in loc.get(\"children\", [])]\n",
    "        location_meta[loc_title.lower()] = {\n",
    "            \"location_id\": loc_id,\n",
    "            \"location_title\": loc_title,\n",
    "            \"location_url\": loc_url,\n",
    "            \"location_rooms\": rooms\n",
    "        }\n",
    "    return location_meta\n",
    "\n",
    "location_meta = build_location_metadata(JSON_PATH)\n",
    "\n",
    "# --- LOAD IMAGE PATHS AND ASSIGN LOCATION METADATA ---\n",
    "image_paths = []\n",
    "payloads = []\n",
    "\n",
    "for loc_dir in IMAGE_ROOT.iterdir():\n",
    "    if not loc_dir.is_dir():\n",
    "        continue\n",
    "    loc_name = loc_dir.name.lower()\n",
    "    meta = location_meta.get(loc_name)\n",
    "    if not meta:\n",
    "        print(f\"Warning: Location folder '{loc_dir.name}' not found in JSON metadata\")\n",
    "        continue\n",
    "    for img_path in loc_dir.glob(\"*.*\"):\n",
    "        if img_path.suffix.lower() not in [\".png\", \".jpg\", \".jpeg\"]:\n",
    "            continue\n",
    "        payloads.append({\n",
    "            \"image_path\": str(img_path),\n",
    "            \"location_id\": meta[\"location_id\"],\n",
    "            \"location_title\": meta[\"location_title\"],\n",
    "            \"location_url\": meta[\"location_url\"],\n",
    "            \"location_rooms\": meta[\"location_rooms\"]\n",
    "        })\n",
    "        image_paths.append(img_path)\n",
    "\n",
    "# --- GENERATE EMBEDDINGS ---\n",
    "def generate_embeddings(image_paths, batch_size=BATCH_SIZE):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size)):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        images = [Image.open(p).convert(\"RGB\") for p in batch_paths]\n",
    "        with torch.no_grad():\n",
    "            inputs = processor(images=images, return_tensors=\"pt\", padding=True)\n",
    "            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "            features = model.get_image_features(**inputs)\n",
    "            features = torch.nn.functional.normalize(features, dim=-1)\n",
    "            embeddings.append(features.cpu().numpy())\n",
    "    return np.concatenate(embeddings)\n",
    "\n",
    "embeddings = generate_embeddings(image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a625f6",
   "metadata": {},
   "source": [
    "## Store embedding in database (only run once when storing vector embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f04f9eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upsert of 954 points...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76040f9810dc4effb21f9f3001d18a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 50/954 (5.2%)\n",
      "Progress: 100/954 (10.5%)\n",
      "Progress: 150/954 (15.7%)\n",
      "Progress: 200/954 (21.0%)\n",
      "Progress: 250/954 (26.2%)\n",
      "Progress: 300/954 (31.4%)\n",
      "Progress: 350/954 (36.7%)\n",
      "Progress: 400/954 (41.9%)\n",
      "Progress: 450/954 (47.2%)\n",
      "Progress: 500/954 (52.4%)\n",
      "Progress: 550/954 (57.7%)\n",
      "Progress: 600/954 (62.9%)\n",
      "Progress: 650/954 (68.1%)\n",
      "Progress: 700/954 (73.4%)\n",
      "Progress: 750/954 (78.6%)\n",
      "Progress: 800/954 (83.9%)\n",
      "Progress: 850/954 (89.1%)\n",
      "Progress: 900/954 (94.3%)\n",
      "Progress: 950/954 (99.6%)\n",
      "Progress: 954/954 (100.0%)\n",
      "Completed storing 954 embeddings in film_locations\n",
      "Collection now contains count=954 points\n"
     ]
    }
   ],
   "source": [
    "# --- STORE EMBEDDINGS IN QDRANT ---\n",
    "client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
    "\n",
    "if not client.collection_exists(COLLECTION_NAME):\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=embeddings.shape[1],\n",
    "            distance=models.Distance.COSINE,\n",
    "            on_disk=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "points = []\n",
    "for emb, payload in zip(embeddings, payloads):\n",
    "    points.append(models.PointStruct(\n",
    "        id=hashlib.md5(str(payload[\"image_path\"]).encode()).hexdigest(),\n",
    "        vector=emb.tolist(),\n",
    "        payload=payload\n",
    "    ))\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "total_points = len(points)\n",
    "uploaded = 0\n",
    "print(f\"Starting upsert of {total_points} points...\")\n",
    "for i in tqdm(range(0, total_points, BATCH_SIZE)):\n",
    "    client.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=points[i:i+BATCH_SIZE],\n",
    "        wait=True\n",
    "    )\n",
    "    uploaded += len(points[i:i+BATCH_SIZE])\n",
    "    print(f\"Progress: {uploaded}/{total_points} ({uploaded/total_points:.1%})\")\n",
    "\n",
    "print(f\"Completed storing {total_points} embeddings in {COLLECTION_NAME}\")\n",
    "print(f\"Collection now contains {client.count(COLLECTION_NAME)} points\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
